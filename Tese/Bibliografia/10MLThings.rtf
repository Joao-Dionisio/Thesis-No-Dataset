{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang2070{\fonttbl{\f0\fnil\fcharset0 Calibri;}{\f1\fnil\fcharset2 Symbol;}}
{\*\generator Riched20 10.0.18362}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs22\lang22 Rudin et. al present a series of 10 mistakes and advices for being more successful in Data Analysis. The paper is divided into each of these 10 parts.\par

\pard 
{\pntext\f0 1.\tab}{\*\pn\pnlvlbody\pnf0\pnindent0\pnstart1\pndec{\pntxta.}}
\fi-360\li720\sa200\sl276\slmult1 There are at least four main families for supervised learning\par
{\pntext\f0 2.\tab}For many domains, all machine learning methods perform similarly.\par
{\pntext\f0 3.\tab}Neural Networks are hard to train and weird stuff happens whe you try to train them.\par
{\pntext\f0 4.\tab}If you don't use an interpretable model, you can make bad mistakes.\par
{\pntext\f0 5.\tab}Explanations can be misleading.\par
{\pntext\f0 6.\tab}You can almost always find an acurrate-yet-interpretable model.\par
{\pntext\f0 7.\tab}Special properties such as decision making or robustness must be built in, they don't happen on their own.\par
{\pntext\f0 8.\tab}Casual inference is different than prediction.\par
{\pntext\f0 9.\tab}There is a method to the madness of deep neural architectures.\par
{\pntext\f0 10.\tab}Artificial Inteligence can't do everything.\par

\pard\sa200\sl276\slmult1\par
\b 1. There are at least four main families of machine learning models for supervised learning \par
\b0 Classification is concerned with the problem of answering yes/no questions while regression\-\-\-\-\-\-\-\-\-\-\-\- tries to attribute a number to some question. Supervised learning algorithms have 4 main families. Logical models (eg. decision trees, rule-based model), Linear combination of trees, stumps or other kinds of features (eg. logistic regression, boosting, random forests, additive models), Casebased reasoning, including K-neares neighbors and kernel based methods (eg. support vector machines with gaussian kernels, kernel regression) and iterative summarization (neural networks). The author presents an example which shows that these models' accuracy can vary widely with the parameters, and that may times they are all about the same.\par
In regards to overfitting, the mathematical field of \i statistical learning theory\i0  implies that Occam's Rule is generally the way to go. Given two equally accurate models, the simplest one is the way to go. The paper also states the Occam's razor bound, an upper bound on the True Error, based on the empirical error, the size of the training set and the size of the function class. This only works, of course, when the size of the function class is finite. When this is note the case, the term reserved to the size of the class can be replaced with a measure of the class' complexity (eg. Vapnik.Chervonenkis dimension, Rademacher complexity).\par
Parameter Tuning can have a drastic impact in the final result and its importance cannot be overlooked. Cross validation is regularly used for tuning parameters, but it can get extremely expensive, the more parameters you have to tune. Some algorithms are more sensitive to tuning than others. While Neural Networks and Random Forests can be difficult to tune, Boosted decision trees seem to reliably generate good results, regardless of tuning.\par
\b 2. All machine learning methods perform similarly (with some caveats)\b0  \par
While NN have received widespread attention due to their performance on computer vision, several papers (\i cited\i0 ) argue that for many machine learning problems, the learning methods perform similarly. This is more evident when, if the methods are properly tuned, the predictors hae an inherent meaning (like age or gender), rather than raw measurements (like pixel values from images and time points from a sound file). One possible explanation is that Computer vision problems are inherently different, since the pixels share a common structure that can be used by the algorithm.\par
According to the author, adding more data, adding domain knowledge, improving the quality of the data, are all thing that can often be more valuable than changing algorithms or tuning procedures. The author avdvises the following. If the predictors have inherent meaning (eg. age and gender), try different algorithms. If, after paremeter tuning, they perform similarly, use the simplest or most meaningful one. Analyze the model and try to embed domain knowledge into the next iteration of the algorithm. If, however, the predictors are raw measurements or there's a big difference between the algorithms, one should try Neural Networks. Additionally, we might be able to find a NN that has already been trained for a problem similar to ours.\par
\b 3. Neural networks are hard to train and weird stuff happens\par
\b0 Model optimization is incredibly painful for NN when compared to many other algorithms, that are typically well-behaved with excelent high-level packages. This makes it so that the optimization for most algorithms comes as an afterthought after the model specification. For optimization, these algorithms often have differentiable convex functions, that can use gradient descent (eg. boosting, random forests, ogistic regression) or greedy methods (eg. decision trees) to optimize themselves or even search techniques (eg. Nearest Neighbour). There is a lot of theory for well-formed convex problems, but since NN's are non-convex and typically non-smooth, this theory doesn't apply and is frequently ignored. In the end, in sharp contrast with the convex setting of many traditional methods, for NN how we choose to optimize impacts the type of solution and generalization performance achieved.\par
NN's performance is also dependent on the initial parameters that, in new applications, is many times randomly chosen. What ends up being done is initializing the network with a pretrained model. When it comes to lack of data, in image processing, data augmentation can be achieved through shifts, reflexion or zooms of the original data.\par
In the end, while deep learning provides interesting results an techniques, there is a lot of missing theory to back up its use. Fortunately, open source projects are common in this area, which can help researches who are new to the field.\par
\b 4. If you don\rquote t use an interpretable model, you can make bad mistakes.\par
\b0 Black box models can create a lot of problems, due to their nature. These types of methods, because they lack the human understanding, can make dangerous predictions because of undetected erros on the training sets, for example. The article states a deep NN that was supposed to classify xray images, which it was doing with great success,  was instead "reading" the writing of the xrays, that detailed the type of xray it was. While humurous, this example serves to show the dangers of black box models. If this model were to be released into the wild, it would be a complete failure, which could have had dangerous consequences. In essence, there is a lack of trust in black box models and it's probably for the better. The author differentiates between low stake decisions, where black box models are acceptable, and high stake decisions, where even one false negatie or false positive can have disastrous consequences. \par
Another interesting example is the COMPAS a proprietary algorithm to determine if an inmate is fit for parole, has 137(!!!!!!) factors that need to be manually entered, leading of course to errors in the data that have lead to denying of parole. \par
\b 5. Explanations can be misleading and we cannot trust them\par
\b0 Probably due to the previous point, people have been trying to explain black box models. This is usually done by building an interpretable model that approximates the black box one. This, however, does nothing in the way of explanation. They also fall short, most of the time, because a model can apporximate another very well, but have quite different underlying assumptions, which could lead to divergences in the future. There are other reasons why black box explanations either don't work or are useless. \par

\pard{\pntext\f1\'B7\tab}{\*\pn\pnlvlblt\pnf1\pnindent0{\pntxtb\'B7}}\fi-360\li720\sa200\sl276\slmult1\i Fidelity\i0 : If an explanation can ever truly capture the decisions of a black box model, then there is no need for a black bix model, we can use the explanation. If, on the other hand, the explanation fails to explain the model, then the explanation is untrustworthy and thus the model is untrusworthy as well.\par
{\pntext\f1\'B7\tab}\i Double Trouble\i0 : By requiring an explanation model next to the black box model, one has to troubleshoot two models rather than one.\par
{\pntext\f1\'B7\tab}\i Insufficient or confusing explanations\i0 : Sometimes the explanations provide little information about how the black box made its predictions. (ok...)\par

\pard\sa200\sl276\slmult1\b 6. It is generally possible to find an accurate-yet-interpretable model, even for neural networks\par
\b0 In many applications, there exists an interpretable model that can rival the best black box models. The issue is that the interpretable models are much harder to develop, when we aim for such accuracy. Nevertheless, regularly this doesn't change the objective value and yet make the model more interpretable and make it easier to use. As an example, compare a NN that determines the risk of a patient in an ICU suffering from a stroke, vs a point based system that does the same thing. The point based system would have a lot of algorithmic knowledge supporting it, but wouls also make the decision transparent. The author points out that interpretability deosn't need to mean the same thing across different domains.\par
There are some approaches that atempt to make NN's more interpretable, namely the "This Looks Like That" (TLLT), that classifies an image based on how similar its parts are to another thing. When trying to predict the species of a bird, it checks its training data to find those that best match it. The head of the test bird might be similar to that of a training bird, etc. Other efforts to make NN's more transparent have also been made.\par
\b 7. Special properties such as decision making, fairness, or robustness must be built in\par
\b0 It's much harder to add features like fairness (as in gender discrimination) while the algorithms learns, than at the end, if all we were concerned with was minimizing a loss function. It's a pretty self explanatory section. The last paragraph sums it up well.\par
"Overall, machine learning methods are now at the point where many software packages are able to train models to optimize standard metrics. The training procedures and model selection techniques are well-matched to this goal. However, it is important to remember that these methods are typically only designed to match that goal, and one should consider how well those metrics are matched to the scientific or predictive question being asked of the data."\par
\b 8. Causal inference is different than prediction (correlation is not causation)\par
\b0 In order to do Machine Learning and Causal Analysis properly, one must not ignore the assumptions that are being made, in regards to cause and effect, of dependence between variables, etc. Another self-explanatory section.\par
\b 9. There is a method to the madness of deep neural network architectures, but not always\par
\b0 THe Machine Learning community has taken efforts into developing a series of building blocks for NN's, that are used to construct a complete architecture. While the theory behind them isn't fully realized, it's getting there, and there also exists a lot of empirical evidence and heuristics toback up their success. CNN's benefict from this technique. As was mentioned previously, the use of pre-trained networks, where the weights have been carefully tuned, is also very popular.\par
\b 10. It is a myth that machine learning can do anything\par
\b0 In the past years, ML has been gaining more and more attention as the number of scenarios where it outperforms humans increases. However, ML is not the answer to everything. One of its issues is the increasing need for data, that we currently do not have. Besides, we don't really know how representative our data is. Models that perform very well on the test sets, can be taken advantage of, in the real world, due to their, let's say, naivite. Stickers on stop signs fool self-driving cars, changing one single pixle changes the prediction of a top of the line model, etc. For this reason, recent efforts have been made in developing a "game" between a generator and a discriminator. The generator's goal is to feed data to the discriminator, both real world data and made up, fake data. The job of the discriminator is to tell them apart. A good generator will make the two groups almost indistinguishable, while a good discriminator will be able to do so. There i, however, concern that good generators could be damaging to society, in the form of fake news, for example. There are many of us that would make for good distinguishers.\par
}
 